{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pf7qnDN5FPe",
        "outputId": "6f3ba8f4-de66-4d57-9625-711866b68189"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7acd35031790>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# classic 42\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        \"\"\"\n",
        "        state_dim:  4 in CartPole\n",
        "        action_dim: 2 in CartPole, 1,0\n",
        "        hidden_dim: just a hidden layer\n",
        "        \"\"\"\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        # 1) shared layers\n",
        "        self.shared_fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 2) policy_head without softmax\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        # 3) value head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        feedford:\n",
        "          input x: shape [batch_size, state_dim]\n",
        "          return (logits, value)\n",
        "          logits: [batch_size, action_dim], actor\n",
        "          value:  [batch_size, 1], Critic\n",
        "        \"\"\"\n",
        "        shared_features = self.shared_fc(x)         # share layer\n",
        "        logits = self.policy_head(shared_features)  # actor\n",
        "        value = self.value_head(shared_features)    # critic\n",
        "        return logits, value"
      ],
      "metadata": {
        "id": "8PzoFhzb5aUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        # store data from game play\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []  # sign for stop\n",
        "        self.values = []\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"\n",
        "        clear for next round\n",
        "        \"\"\"\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "        self.values = []"
      ],
      "metadata": {
        "id": "G4SwiWky7fxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim,\n",
        "                 lr=3e-4, gamma=0.99,\n",
        "                 K_epochs=4, eps_clip=0.2):\n",
        "        \"\"\"\n",
        "        state_dim\n",
        "        action_dim\n",
        "        lr: learnign rate\n",
        "        gamma: discount factor\n",
        "        K_epochs: how many rounds to train for each sampling\n",
        "        eps_clip:clip range in objective function\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        # 1) policy to renew\n",
        "        self.policy = ActorCritic(state_dim, action_dim)\n",
        "\n",
        "        # 2) optimizer\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "        # 3) load old plicy and send parameters to new\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # 4) MSE for values\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "        #\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        use policy_old to interact with envirment without grad update.\n",
        "        reture: (action, logprob, value)\n",
        "        \"\"\"\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)  # shape: [1, state_dim]\n",
        "        #dont update just get result\n",
        "        with torch.no_grad():\n",
        "            # 1)get logit and V from old policy\n",
        "            logit, value = self.policy_old(state)\n",
        "\n",
        "            # 2) get action posibility distrubution from old policy\n",
        "            policy_dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "            # 3) select a sample from distrubution\n",
        "            action = policy_dist.sample()\n",
        "\n",
        "            # 4) log π(a|s)\n",
        "            action_logprob = policy_dist.log_prob(action)\n",
        "\n",
        "        # return\n",
        "        return action.item(), action_logprob.item(), value.item()\n",
        "\n",
        "    def update(self, buffer: RolloutBuffer):\n",
        "\n",
        "        # change data in buffer to tensor\n",
        "        states = torch.FloatTensor(buffer.states)\n",
        "        actions = torch.LongTensor(buffer.actions)\n",
        "        old_logprobs = torch.FloatTensor(buffer.logprobs)\n",
        "        rewards = buffer.rewards\n",
        "        is_terminals = buffer.is_terminals\n",
        "        values = torch.FloatTensor(buffer.values)\n",
        "\n",
        "        # get the discounted_rewards\n",
        "\n",
        "        discounted_rewards = []\n",
        "        G = 0\n",
        "        for reward, done in zip(reversed(rewards), reversed(is_terminals)):\n",
        "            if done:\n",
        "                G = 0\n",
        "            G = reward + self.gamma * G\n",
        "            discounted_rewards.insert(0, G)\n",
        "\n",
        "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
        "\n",
        "        # normalise discounted_rewards\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / \\\n",
        "                             (discounted_rewards.std() + 1e-8)\n",
        "\n",
        "        # get advantages (discounted_rewards - values)\n",
        "        advantages = discounted_rewards - values\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "        # update with (K_epochs) clip-based\n",
        "        for _ in range(self.K_epochs):\n",
        "            # (a)  forward with policy new\n",
        "            logits, state_values = self.policy(states)\n",
        "            policy_dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "            # (b) policy new log_probs\n",
        "            new_logprobs = policy_dist.log_prob(actions)\n",
        "\n",
        "            # (c) get ratio between policy old and policy new\n",
        "            ratios = torch.exp(new_logprobs - old_logprobs)\n",
        "\n",
        "            # (d)  surr1 as classic RL objective , surr2 as clip of that classic RL objective is change between old and new is too big\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
        "\n",
        "            # (e) max[ratio×adv,clamp(ratio)×adv]\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            # (f) value loss (MSE)\n",
        "            value_loss = self.mse_loss(state_values.squeeze(), discounted_rewards)\n",
        "\n",
        "            # (g) add policy_loss and value_loss as total loss\n",
        "            loss = policy_loss + 0.5 * value_loss\n",
        "\n",
        "            # (h) lets go\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # update on policy_old\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
      ],
      "metadata": {
        "id": "2PIK9Hsk70Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ppo_cartpole():\n",
        "    # 1) get env\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    # 2) get dmention\n",
        "    state_dim = env.observation_space.shape[0]   # CartPole: 4\n",
        "    action_dim = env.action_space.n             # CartPole: 2\n",
        "\n",
        "    # 3) PPO agnet  & RolloutBuffer\n",
        "    ppo_agent = PPO(state_dim, action_dim, lr=3e-4, gamma=0.99, K_epochs=4, eps_clip=0.2)\n",
        "    buffer = RolloutBuffer()\n",
        "\n",
        "    # 4)\n",
        "    max_episodes = 3000       # play 3000 games\n",
        "    max_timesteps = 200       # each game 200 moves\n",
        "    update_timestep = 2000    # update once 2000 moves\n",
        "    print_freq = 100           # print\n",
        "\n",
        "    # record\n",
        "    timestep = 0\n",
        "\n",
        "    for episode in range(1, max_episodes+1):#3000 games\n",
        "        state = env.reset(seed=seed)  # reset env\n",
        "        total_reward = 0\n",
        "\n",
        "        # inner loop: 200 times\n",
        "        for t in range(max_timesteps):\n",
        "            timestep += 1\n",
        "\n",
        "            # a) input state, get action, logprob, value\n",
        "            action, logprob, value = ppo_agent.act(state)\n",
        "\n",
        "            # b) input action, get next_state, reward, done, info\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # c) save orbit on buffer\n",
        "            buffer.states.append(state)\n",
        "            buffer.actions.append(action)\n",
        "            buffer.logprobs.append(logprob)\n",
        "            buffer.values.append(value)\n",
        "            buffer.rewards.append(reward)\n",
        "            buffer.is_terminals.append(done)\n",
        "\n",
        "            # d) move on\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            # e) update ppo_agent every 2000\n",
        "            if timestep % update_timestep == 0:\n",
        "                ppo_agent.update(buffer)\n",
        "                buffer.clear()           # clear old buffer\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # print\n",
        "        if episode % print_freq == 0:\n",
        "            print(f\"Episode: {episode}, Reward: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Let go!!!\n",
        "    train_ppo_cartpole()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7DcVdlzFqdD",
        "outputId": "7fbef446-285b-43f3-9212-2c681c7bca34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100, Reward: 15.0\n",
            "Episode: 200, Reward: 19.0\n",
            "Episode: 300, Reward: 24.0\n",
            "Episode: 400, Reward: 15.0\n",
            "Episode: 500, Reward: 25.0\n",
            "Episode: 600, Reward: 13.0\n",
            "Episode: 700, Reward: 78.0\n",
            "Episode: 800, Reward: 27.0\n",
            "Episode: 900, Reward: 69.0\n",
            "Episode: 1000, Reward: 98.0\n",
            "Episode: 1100, Reward: 71.0\n",
            "Episode: 1200, Reward: 164.0\n",
            "Episode: 1300, Reward: 51.0\n",
            "Episode: 1400, Reward: 141.0\n",
            "Episode: 1500, Reward: 200.0\n",
            "Episode: 1600, Reward: 200.0\n",
            "Episode: 1700, Reward: 200.0\n",
            "Episode: 1800, Reward: 179.0\n",
            "Episode: 1900, Reward: 200.0\n",
            "Episode: 2000, Reward: 200.0\n",
            "Episode: 2100, Reward: 200.0\n",
            "Episode: 2200, Reward: 200.0\n",
            "Episode: 2300, Reward: 200.0\n",
            "Episode: 2400, Reward: 200.0\n",
            "Episode: 2500, Reward: 200.0\n",
            "Episode: 2600, Reward: 200.0\n",
            "Episode: 2700, Reward: 200.0\n",
            "Episode: 2800, Reward: 200.0\n",
            "Episode: 2900, Reward: 200.0\n",
            "Episode: 3000, Reward: 200.0\n"
          ]
        }
      ]
    }
  ]
}